% !TEX root = ../Thesis.tex

\chapter{Related work}
\label{c:related}

\todoMissing{Cite from Scientific Writing report \cite{hennemann_sw_2021} }
\todoMissing{Cite from Marco Dis \cite{vogt_dis_2022} }

This chapter aims to provide a background for the given topics as well as talking about the related and recent research 
activities in this field. In the context of this thesis, we consider five topics, which are necessary to establish 
freshness-awareness inside a Database Management System (DBMS).
It is therefore separated into five sections, were each part contributes to a characteristic, needed to 
provide Data Freshness. This includes a definition of Data Freshness in general ($\rightarrow$ Section \ref{sec:definition}), 
possible metrics to define cost functions which are used
in the evaluation process as different freshness-levels are expressed and compared ($\rightarrow$ Section \ref{r:freshness_metrics}), 
how to involve different versions of data items ($\rightarrow$ Section \ref{sec:consistency}). 
Furthermore, which possibilities exist on how to replicate data ($\rightarrow$ Section \ref{sec:replication}) 
and propagate updates to outdated nodes to refresh them.
Finally, this is concluded how users can actually specify their tolerated levels of freshness ($\rightarrow$ Section \ref{r:read}).



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\todoMissing{Check necessity}

To ensure the overall consistency of a system, current approaches in cloud environments use well-established protocols with unified blocking behaviours. 
This includes Strict two phase locking (SS2PL) for correct serializability treatment as well as two-phase commit (2PC) for global atomicity in a distributed database 
setup. Such mechanisms are mainly utilized when updates and changes to the system are replicated eagerly to every available replica to maintain a consistent state
while complying with the common ACID properties \cite{tamer:2005}.
However, these  directly impact and ultimately mitigate the availability and response time 
agreements of most service providers.
Hence, different approaches were introduced which aim to relax the strict \textit{ACID} properties towards 
a \textit{BASE} assumption \cite{base2008}.
\textit{BASE} in this context stands for \textbf{B}asically \textbf{A}vailable \textbf{S}oft 
State and \textbf{E}ventual Consistency \cite{shapiro:2011}.
To implement this concept, updates on data items do not need to be executed immediately on every participating node.
It is sufficient to apply the change only on a few nodes and deferring the data replication to a point in time when the workload on the system is low and 
resources are not occupied by client requests. This form of lazily replication can drastically increase the performance while also reducing the costs of maintaining
all replicas at once. However, since these client requests are not serialized anymore this could lead to users accessing outdated and stale data items
which have not been updated yet.
As a consequence of lazy replication the system now persists multiple versions of data objects which logically reflect the data freshness.
Voicu et al. \cite{voicu:2010} suggest that since not all applications need the most up-to-date data, one could easily exploit 
this side effect by keeping several replicas of a datum as different levels of freshness.
This notion of or recency can then be utilized to ease the selection of correct replicas to fulfil individual client requests which even tolerate stale data as well.
For cloud providers this combination of eager and lazy replication together with the resulting different stages of freshness 
offers a great trade off between latency and access time for freshness and enables efficient usage of available resources.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Data Freshness}
\label{sec:definition}
The consideration of data freshness is a widely used concept among database systems and intuitively introduces the idea of how old or stale data is.
Nonetheless, there is no commonly agreed defintion, hence several different notions and interpretations exist, on how to characterize and measure data freshness.
This was already described by Naumann et al. \cite{naumann:1999} in 1999, which further state that despite these variations, it is strongly related to the concept of accuracy. 
The accuracy of a data object could therefore be loosely summarized as the percentage of objects without data change. 
This thought was also shared by the author of \cite{redman:1996} who defined the data accuracy as "the degree of agreement between 
collection of data values and a source agreed to be correct".
Hence, it can be considered as the precision of such data elements in respect to their most up-to-date version.


\todoMissing{Check}
This approach is now commonly used among distributed systems which compare replicas with a designated "single source of truth" or some defined 
primary or leader nodes in such a setup. Referring to freshness as a measure of the divergence between a replica and the current value \cite{cite}.
Such strong application driven requirements call for a dedicated data model which represents the handling of data in a system which has
freshness related requirements on arbitrary data items.


Peralta \cite{peralta:2006} also summarized freshness as an implication of how old data is and referred this to an user's expectation on the actuality of an object. 
Consequently, using the last time objects were updated, to identify any accuracy measures. Peralta also distinguished between two data quality factors. 
The \emph{Currency factor} which expresses how stale data is after it has been extracted and finally been
delivered to the user. This concept is often considered within Data Warehousing systems, when already extracted materialized data is processed and delivered to the user.
Because the extracted source data might have already been updated after it has initially been extracted, consequently resulting in stale data to be del√∂ivered.
The second factor corresponds to the \emph{timeliness} of data, which essentially states how old data is and captures the gap between the last update and the actual delivery.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Freshness Analysis and metrics}
\label{r:freshness_metrics}
A metric is a specific figure which can be used to compare and evaluate given quality factors with each other.
Along the various definitions and approaches to define data freshness itself, the metrics to identify and measure the actual degree of freshness also varies.
Several metrics are summarized by \cite{cho:2000, pacitti:2000, peralta:2006} and can be categorized as:
\begin{itemize}
    \item \textbf{Currency metric}: Measures the elapsed time since the source data changed without being reflected in a materialized view or a replica in distributed setups.
    \item \textbf{Obsolence metric}: Which accumulates the number of updates to a source since the data extraction time. For replicated systems this can be characterized as the 
    number of pending updates that still waiting to be applied to a secondary system after it has been refreshed.
    \item \textbf{Freshness-ratio metric}: Identifies the percentage of extracted elements that are indeed up-to-date compared to their current values.
    \item \textbf{Timeliness metric}: Measures the time that has elapsed since the data was updated. Generally defined as the time when the replica was last updated.
\end{itemize}


Furthermore, \cite{bedewy:2016} and \cite{zhong:2018} define the notion of freshness by associating it to the \emph{age-of-information} (AoI).
This is defined as the timeliness represented by the timestamp of the transactions which has updated the value. 
Which is enriched further with the \emph{age-of-synchronization} (AoS) that corresponds to the time when the value was actually updated.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Freshness Constraints}
\label{r:express_freshness}

Along the various freshness metrics as described in \ref{r:freshness_metrics}, Tamer et al. \cite{tamer:2005} list three types of freshness constraints
to suggest a tolerated level of freshness.
\begin{itemize}
    \item \textbf{Time-bound constraints}: Are used as timestamps that accept values which have been updated by younger transactions. 

    \item \textbf{Value-bound constraints}: This is commonly used with numbers than with text and considers the percentual deviations from the current value.

    \item \textbf{Drift constraints on multiple data items}: These are especially relevant for transactions which read multiple data items. Tolerates if possible 
    update timestamps of all required data items are all within the same specified interval. Additionally, for aggregations, as long as a particular aggregate computation 
    is within a tolerable range it can be accepted. Even if some individual values of the data items are more out of sync than the compound operation.
\end{itemize}

The notion of time-bound constraints is also shared by the authors of \cite{voicu:2010}. They propose to measure and specify these freshness constraints 
with the notion of \emph{absolute-} and \emph{delay freshness} to characterize their proposed freshness functionalities.

Here the \textit{Absolute Freshness} of a data item \textit{d} is characterized by the commit timestamp \textit{t} 
of the most recent update transaction that has modified the item \textit{d}.
These timestamps can be used by the client to individually define their freshness requirements. The younger a timestamp the fresher the data.
Additionally, they use \textit{Delay Freshness} to define how old and outdated requested data objects $t(d)$ are compared to the commit time of the current value $t(d_0)$.
Defining their freshness function as 
\begin{equation}
    f(d) = \frac{t(d)}{t(d_0)},  with f(d) \in [0,1]
\end{equation} 
resulting in a \emph{freshness index}.
R√∂hm et al. \cite{rohm:2002} state that such an index consequently reflects how much the data has deviated from its up-to-date version.
An index of \emph{1} intuitively means that the data is at its most recent state, while an index of \emph{0} is defined as infinitely outdated.

While \cite{xiang:2008} and \cite{fekete:2018} both consider the delay-based staleness in the time domain as well, they also consider constraints on 
an acceptable value-based divergence $\delta$. This aims to measure the difference between two numerical values by analyzing their similarity on basis of their 
absolute value $|d_0 - d| < \delta $. Low values between base and replicated data reflects a more up-to-date replica for a given object.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Update propagation}
\label{r:replication}
An essential part to gain performance with freshness-aware data management is loosening the update situations by reducing the number of 
replicas which need to be updated. This decoupling between necessary updates on a primary site and deferred updates towards secondaries 
reduces the total update time and in contrast increases the overall availability of the database system ~\cite{quorums:2003}.
This mechanism itself suggests to the user that the database system is updated eagerly while internally the updates are actually propagated lazily. 
Although this has the side effect that the secondaries hold slightly outdated or stale data.
Due to the decoupling these can now efficiently be utilized as read-only copies to speed up OLAP workload while still performing transactional processing 
at primary level as \cite{psaroudakis:2015, rohm:2002, xiang:2008} have pointed out in their work.

Depending on the given system architecture, different refresh-strategies have been proposed on how updates are propagated towards outdated nodes.

Wei et al. \cite{wei:2004} propose to replicate data in form of an update policy adaptation algorithm, that dynamically selects update policies
for derived data items. Which are then executed based on internal system conditions. These conditions are defined as several layers of validation a transaction has to pass.
E.g. in the validation stage, the system checks the freshness of the accessed data. If this accessed data is not fresh enough the entire transactions will be restarted.
This imposes a huge performance mitigation and wastes ressources since the transaction itself could be processed and re-queued multiple times before it might actually get executed.\\

Psaroudakis et al. \cite{psaroudakis:2015} mention the existing design gaps between OLTP- and OLAP-oriented database management systems and describe the importance of 
allowing workloads to be executed in parallel. However, they solely focus on a single table level rather than a replication scenario in a distributed 
environment. Furthermore, they are interested in real-time processing and claim that even the slightest outdated data is unacceptable to provide real-time reporting.
To fulfil these mixed workload requirements they summarized the idea of \emph{SAP HANA}, which offers a main area that contains the base data and a delta area 
which supports transactional operations and includes recently changed data. Read operations therefore need to query both main and delta area jointly to provide results. 
Since the delta area could increase without bound it is periodically merged into the main section. 


A similar approach is shared in \cite{wei:2004} which tried to mitigate the complexity and replication overhead by combining base data elements witth derived elements.
For queries, a base set of information is enriched with delta fragments to derive the relevant output.
Since the base information is always available and queued changes are applied during runtime to recompute 
the actual response, this will reduce the needed update cycles on all replicas. Nonetheless, this approach has a higher cost when encompassing several derived data sets, and 
is therefore rather suited for values that can easily be derived.


To increase the transactional throughput Pacitti et al. \cite{pacitti:2005} introduce concurrent replica refreshments.
They discuss the idea of a \emph{Preventive Replication} algorithm by using an asynchronous primary-copy replication approach while still being able to enforce
strong consistency. This is achieved by utilizing a First-In First-Out Queue (FIFO) queue to correctly serialize any updates which shall be delivered to secondary replicas.

The authors in \cite{voicu:2010} propose several multi-tier layers to handle replication and freshness scores. 
Nodes are classified into two categories either as updatable or read-only and placed onto three levels.
Level-1 nodes receive all updates immediaetely acting as the primary servers, level-2 nodes are read-only nodes containing as up-to-date data as possible while the
level-3 read-only nodes are updated rather infrequently. These layered levels are composed as a tree receiving asynchronous updates from the lower level. 
In this case the higher the level, the more outdated the data.

In contrast, \cite{xiang:2008} approaches its replication pursuit by establishing a relationship between individual data items, represented as a directed acyclic graph (DAG).
Their graph essentially denotes a dependency between data items. The root node of such graphs corresponds to a base data item and child nodes correspond 
to its deviations over time, which as well corresponds to a multi-layer replication setup.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Refresh Strategies}
\label{r:strategies}
Based on the lazy propagation algorithms presented in \ref{r:replication}, three refresh and different propagation strategies could be identified when updates should be propagated 
to replicas in order to refresh them.

Despite that the authors of \cite{psaroudakis:2015} rather focused table-level objects and not on comprehensive system replication scenarios, the up-date mechanism 
still follows the same requirements. To jointly support a mixed workload of OLTP- and OLAP-oriented transactions,
it suggests a periodic approach to merge the main and delta areas. This essentially refers to updating an outdated base item, in this case the main area.

R√∂hm et al. \cite{rohm:2002} avoid scheduling such update propagations completely. They suggest to simply decouple the primary update transactions from the read-only nodes
and immediately executing the propagation afterwards. Although this aids the throughput of the initial update transactions, the secondaries 
stay outdated for almost no time at all, leaving no room for possible outdated replicas with different versions to even exit. 
Furthermore, such approaches as well as periodic scheduling completely neglect to verify the current load on the underlying system.
Since this could unnecessarly cause performance mitigations on the replicas to be updated, it should always be accompanied by identifying the idle time of replicas as well 
as ensuring that the current load on the replicas is not too high and is therefore able to endure such an update \cite{voicu:2010}.

In contrast to \cite{peralta:2006} which determine the goal to identify the minimum set of refresh transactions, such that it is guaranteed that a node contains 
sufficiently fresh data with respect to the users requirements. Therefore, any outdated node will independently pull the number of updates which are necessary to 
fulfil future requests along the requirements.

Wei et al. \cite{wei:2004} follow a different approach by analyzing the \emph{Access Update Ratio} (AUR) for any data object \textit{d}. 
\begin{equation}
AUR(d) = \frac{AccessFrequency(d)}{UpdateFrequency(d)}
\end{equation}. 
As depicted above, any item beyond an AUR of 1 is considered to be accessed at least as frequently as it is updated.
Hence, it is considered as \emph{hot} and shall be updated as soon as possible for applications to receive the real-time value. 
Whereas a ratio below this threshold refers to \emph{cold} data and establishes that this item is accessed comparably infrequently and thus needs no immediate refresh. 




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Consistency}
\label{sec:consistency}
There are multiple constraints to enforce when trying to ensure the consistency in a distributed setup.
The authors of \cite{wei:2004, xiang:2008} discussed data freshness together with the scheduling of update transactions from the 
point of view of real time applications. They ellborated the concept of temporal validity with respect to value-based freshness,
such that specific values are only considered valid for a certain time interval before becoming outdated. Hence, they describe the necessity to classify objects into
temporal data that can continuously change to reflect a real world state, and non-temporal objects that will not become outdated overtime like the validity of an ID card.
This concept essentially pursues the fundamentals of temporal databases, which keep historic values as well as their validity-intervals, to allow the reconstruction of any 
past value \cite{etzion:1998}. 

Voicu et al. \cite{voicu:2010} proposed to decouple transactions in order to correctly separate individual requirements for update propagations.
These transactions are differentiated into four variations. Firstly, regular update transactions that target primary nodes, secondly propagation transactions 
that refresh read-only nodes, followed by refresh transactions that are executed if a freshness level on a local store cannot be provided and 
finally OLAP related read-only transactions.
Furthermore, they require that data accessed within a transaction is consistent, independent of its freshness. To correctly serialize the updates
they ensure that their update serialization order is the commit order of the initial update transactions.\\

The authors of \cite{psaroudakis:2015} introduced a common data structure to provide access for both OLTP and OLAP. 
They enabled the usage of delta and main storage together with the utilization of multi-version concurrency control (MVCC) to allow both workloads to be executed in parallel.
This implicitly enables the system to keep several versions of a data item (see Section \ref{sec:concurrency_control}).
These versions can be directly used as outdated replicas or to provide certain freshness guarantees.
Although this would indeed improve update times and would support referential integrity, the implementation of MVCC is complex and needs more system resources than usual.

Finally, the authors of \cite{akal:2005} propose freshness locks which are applied to the stores which have been selected to fulfil the read requests. 
These locks aim to provide fast response times for OLAP workload and essentially ensure that data is not refreshed aslong as it is being read by another transaction.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Freshness-aware Read Access}
\label{r:read}
Since the fundamental idea of data freshness aims to utilize all provided resources to improve the read access while transactional workload is being executed.
Therefore the read access is a matter of efficiently routing any client request to a suitable replica in respect of the required level of freshness.
According to \cite{rohm:2002, akal:2005} a router needs to utilize system state information on replicas to identify the best way to route a query.

In \cite{voicu:2010} clients are able to contact any read-only replica directly. For every read-only transaction they can as well specify
their tolerated freshness level, by providing a timestamp which is internally converted to a freshness index. If the replica is able to fulfil the request 
it directly returns the response to the client. If it currently does not possess a sufficient freshness level it routes the request to another node which can be identified 
by routing it towards the root of the tree. The parent is then able to identify which node is capable to fulfil the condition.  
If none of the replicas are able to execute the request wthin this subtree, a refresh transaction is triggered, which will update and refresh all nodes before 
processing the read operation.

The authors \cite{huang:2020} introduce a central \emph{preference manager as a service} at the client site. This manager is able to suggest where to route queries 
based on given cost metrics. By comparing individual latencies for any request they aim to improve the overall performance to deliberately choose if a request 
shall be routed to a primary or secondary site. This analysis is influenced by the \emph{Replication Lag} inside a MongoDB cluster which refers to the average time 
that passes before an update is propagated to the secondaries. 




