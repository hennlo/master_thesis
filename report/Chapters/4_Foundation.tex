% !TEX root = ../Thesis.tex
\chapter{Foundations}
\label{c:Foundation}

This chapter describes concepts and general foundations, which are necessary to supplement 
the contents of this thesis. These foundations are mainly associated with topics on distributed data management.


\section{Polystores}

The decision which data structure to use and built upon is a crucial step for the overall performance of a systems design, as suggested by
H.Plattner and B.Leukert ~\cite{plattner2015}.\\
While row-oriented data stores might be useful and preferred for write-heavy transactional 
workloads they are rather insufficient for purely analytical workload which would rather benefit from a
column-oriented data store with less write operations ~\cite{sigmond2008}.\\

Despite the fact that nowadays there exist a variety of Database Management Systems (DBMS) which were originally created with an intention to
support specific scenarios,
applications are getting more complex relying on various requirements and characteristics 
to serve multiple use cases at once.
That is why modern day applications can not solely rely on one storage technology alone. 
Consequently Multi- and Polystore systems have emerged. \\
\\
While multistore database systems aim to combine and manage data across heterogeneous data stores,
polystore systems are essentially based on the idea of combining multistores with
\textit{polyglot persistence} ~\cite{polypheny2020}.
Polyglot persistence is a term which refers to a practice originated from the concept 
of \textit{polyglot programming} or microservice architectures, to utilize different 
programming languages for different task requirements following a best-fit approach ~\cite{fowler2011}. \\
Along this paradigm, polystores want to utilize multiple data storage technologies to
fulfill different needs for different application components in order to cope
with mixed and varying workloads.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Data Partitioning}
\label{sec:part}

Beside the utilization of several data storage engines, the data model and structure 
will have an enormous impact on the overall performance. Depending on the query or 
how data is accessed, data partitioning can be used to increase the efficiency and 
maintainability of the system ~\cite{Agrawal_2004}.\\
The process of data partitioning refers to splitting the data into logically and sometimes even
physically separated fragments.

In general data partitioning can be distinguished between two variations.
Both of these forms split the data into multiple parts.
It is therefore often called fragmentation \todoMissing{Cite}. 

\begin{description}
    \item [Vertical Partitioning] is usually applied during the design of a data model inside a 
    database. This involves the creation of tables with fewer columns and therefore using additional 
    tables to store the remainder of columns ~\cite{vertical_1984}. This approach is often used in the 
    context of normalization of a data model ~\cite{normalization_2012}. In order to combine and reconstruct 
    these vertical partitions again there needs to be some sort of redundantly stored data like the primary key.
    Which uniquely identifies individual items. 
    \item [Horizontal Partitioning] refers to the partitioning of objects like tables 
    into a disjoint set of rows that can be stored and accessed separately ~\cite{horizontal_1982}.
    To support this explicit form of partitioning there exist several partition algorithms.
    The most common ones are List, Range and Hash partitioning. These algorithms can be applied to a
    table based on an arbitrary column which results in a fragmentation of the table 
    based on the data values of the selected column.
\end{description}

Data partitioning generally enables a system to process data concurrently and 
to some extent even in parallel. Considering that access to data can be 
efficiently load balanced and therefore enhances the throughput per query.\\

Although data partitioning is often associated with the improvement of query performance.
It can be also be used to simplify the operating of a DBMS cluster and therefore help 
to increase the overall availability.
Through the replication of partition fragments, the data resilience of the system
can be improved. Even if part of the data storage nodes are temporarily not 
reachable, your system still might be fully operational and available due to the 
replication and distribution of data fragments, which is still one of the main 
pursuits of current data management solutions ~\cite{dbre2017}.




\section{Temperature-aware Data Management}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{CAP Theorem}
\label{sec:cap}
An essential part of distributed systems is the handling of failures or outages of participating components. 
The \emph{CAP theorem} ~\cite{cap2002} introduced by E. Brewer ~\cite{brewer:2000} discusses these scenarios and states, that it is not possible 
to keep the system available while providing global data consistency at the same time.
This problem is driven by the claim that a node within a clustered system cannot identify whether another node or the 
network connection in between has failed (network partitioning).\\
Although this theorem was primarly introduced to support the differentiation between \emph{Availability} and \emph{Consistency}, 
it only formulates the trade-off in case of a failure.
Since this should rather be the exception, Abadi ~\cite{abadi2012} generalized the concepts of Brewer by introducing \emph{PACELC} as an extension to the CAP theorem.
This essentially adds the more common non-failure case to the definition.
He claims that it is not sufficient to reduce the decision on the occurrence of the failure alone. 
Because even in high-available environments the data needs to be replicated to ensure availability and thus latency. 
Therefore, the possibility of failure alone, even in the absence of a failure, implies that the availability depends to some degree also on the data replication.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\section{Data Replication}

In distributed setups the utilization of data replication is crucial to improve the query performance
by replicating certain partitions of data to where it is actually needed ~\cite{cloudpart_2012}.\\
Furthermore, replication also increases the availability of the system and protects it against outages or performance mitigations,
by allowing workload redirection and load balancing to healthy replicas in the cluster ~\cite{quorums:2003}.
However, maintaining the consistency of all available replicas, results in scalability problems.\\
The author of ~\cite{abadi2012} summarizes "three alternatives for implementing data replication".
He states that these different nuances inherently result in the aformentioned trade-offs between availability and consistency described in \ref{sec:cap}.

The system can either choose to send the updates to all replicas at once, send updates to a predefined replica first or to a single arbitrary node.
 While the first approach can by directly applied to a system, the second and the third require the node that has received the initial modification to trigger an additional update operation.
\\
Generally the data replication approaches can be ultimately distinguished into two approaches ~\cite{gray:1996}.

\textbf{Eager Replication} provides a strong consistency among all replicas. Each modification will first be applied on all nodes
before the update transaction is considered to be successful. Hence, no stale data retrieval is possible, and users can choose to query any of the available nodes.
Because the update is however applied synchronously, the transaction blocks until the last replica has finished the write-operation. 
Since this is done within one global transaction, specifically in heterogeneous environments, the performance of an update is bound by the slowest performing node.\\

In contrast to its strict counterpart, \textbf{Lazy Replication} decouples the primary update transaction from the update propagation to secondary nodes.
In its basic form it only supports weak consistency. Since updates only need to be acknowledged and executed by one store, 
the update propagation to the remaining nodes is executed asynchronously ~\cite{fekete:2018}.
Although, this improves parallel processing and increases the availability because only one node is blocked during the update. All other nodes can still serve incoming requests,
which especially in increases the popularity for OLAP-based applications ~\cite{daudjee:2006}
However, during the convergence period, until all changes have been replicated, the system is exposed to an inconsistent state.
As pointed out by ~\cite{cho:2000} utilizing a lazy propagation of updates, immediately leads to different versions of participating data items and thus also provides stale data.
This could result in retrieving outdated data, if the client contacts one of the outdated nodes.
\\
Since the initial transaction defers the update propagation, this approach automatically results in \emph{Eventual Consistency}.
Although not considered strong, this form of weak consistency guarantees, that if no further updates are made 
during convergence, all accesses to these replicas will eventually see the same value and become uptodate. ~\cite{quorums:2003}


\subsection{Eager Replication}
\subsection{Lazy Replication}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Concurrency Control}
 

A major topic in the context of distributed database systems is \emph{concurrency control}~ \cite{bernstein:1981}.

concurrency control ~\cite{bernstein:1987} 

As described in Section \ref{sec:cap}, PACELC deals with the trade-off of latency and consistency in non-failure scenarios.
In this context a major and also extensively studied problem of distributed database systems is concurrency control \todoMissing{Cite}.

- Discuss downsides of SS2PL because eagerly  replicated and not allowing much performance but ensuring high consistency.

\todoMissing{COnvert to bullet points}
\subsection{(SS)2PL}
\subsection{MVCC}
Established for multi-version databases that extend the concept of shadow pages by keeping the complete history or at least multiple versions of each object \cite{bernstein:1982}.
MVCC was introduced by \cite{bernstein:1983}

Snapshot Isolation \cite{daudjee:2006} snapshot isolation in lazy replicated systems is difficult

Snapshot Isolation provides weaker consistency guarantees as \emph{one copy serializability} (1SR) and can therefore increase concurrency by relaxing the isolation requirements.
Snapshot Isolation was introduced in by Berenson et al. ~\cite{berenson:1995}

\subsection{Discussion}
Although, there now exist some work that provides serializable versions of multi-version concurrency control~\cite{faleiro:2015}.

There exists  to make SI serializable as sumamrized by the authors of ~\cite{fekete:2005}
