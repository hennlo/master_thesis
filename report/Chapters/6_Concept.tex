% !TEX root = ../Thesis.tex
\chapter{Concept}
\label{c:concept}

\todoMissing{Check that concept is not bound to any implementation}

In this chapter we will define all functional requirements, necessary to establish freshness-aware data management in general.
We will therefore discuss the contents of section \ref{c:related} and propose solutions how these approaches and techniques can be applied to polystore systems.
Hence, this chapter is separated into several sections were each represents a necessary building block to provide the notion of data freshness.


\section{Functional Requirements}

* Multiple Versions -> Data Replicas
(* Express/Specify Freshness - Query extension) Implementation
* Different Freshness Metrics -> Freshness Metrics 
* Freshness Comparison -> How to compare the freshness
* Update Propagation -> 
* Refresh Strategies
* Consistency -> Updates/Operations need to be applied in same order as primary copy
* Freshness Read
 (extension to a query languages to specify a tolerated level of freshness)


Except the obvious existence of multiple versions per data object (see \ref{sec:data_replicas}), there are several prerequisites and requirements to establish freshness-awareness. 

Essentially we have to think about how we want to express freshness(see \ref{express}), find a suitable metric to measure it (see \ref{sec:metric})
and provide users a possibility to formulate an acceptable level of freshness. Based on these fundamentals we need to consider how update transactions 
can be decoupled and deferred, how replicas will
be refreshed \ref{sec:replication_strategy} while ensuring the consistency \ref{consistency_concept} of the system and how to enable the routing to identify freshness levels to speed up read-only operations. 





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Data Replicas}
\label{sec:data_replicas}

One of the main requirements of data freshness is the necessity and existence of different versions that can receive outdated data. 
Only with these versions we are even able to compare their state and define freshness for a given replica.

As already discussed in \ref{r:strategies} it is crucial to define and assign roles to loosen locking situation on nodes
and minimizing update times over all.
The related work clearly distinguished between primary-update nodes and read-only nodes, where 
read-only nodes cannot be directly updated by the user and will only receive refresh updates internally from the primary nodes.

Since polystores inherently replicate or distribute their data across multiple stores we already have multiple replicas containing the data. 
Because we aim to reduce locking situations by decoupling primary updates from secondary transactions,
we can simply use a lazy replication approach to propagate the updates asynchronously. 
By definition this solution automatically creates multiple versions by deferring the update propagation to secondaries.
This immediately leverages the nature of polystore systems in such a way that no additional replicas have to be artificially created in order 
to support multiple version for each data object, which will eventually converge. 
This replication approach is discussed in more detail in section \ref{sec:refresh_operations}.\\


Since we always need a foundation for all freshness-related comparisons we will need at least one up-to-date replica. 
This replica should contain relevant information to illustrate the deviation from another possibly outdated version on a different node.
To achieve this in a polystore environment we need to be able to classify existing stores into specific groups or roles.
Naively these could be \emph{up-to-date} and \emph{outdated}.\\
On the basis of these roles we are then able to decide which replicas to consider for which use case.
Alongside the idea of a polystore system, where each underlying engine has its own purpose, we can directly apply these roles 
based on the provided use case. E.g. label those stores as up-to-date that support highly transactional workload and will therefore be considered for every update.
And configure replicas as outdated, when they are rather suitable for analytical queries which implicitly harvests the benefits of the encompassed stores. \\

In order to consequently compare those replicas they need to be equipped with metadata of at least two timestamps. 
One is the update timestamp when the replica has been last modified,
the other is the commit timestamp of the original transaction which has modified the replica.
This differentiation is important since it allows us to compare replicas based on their commit timestamp. This timestamp is always directly associated with the primary transaction.
It means that even if the update propagation for secondaries has been deferred to a later point in time, as soon as they converge they will have the same commit timestamp 
as their primary counterpart.
Otherwise, it would not be possible to compare replicas based on their timestamp, since individual commit timestamps per replica would not allow a direct timestamp comparison 
to determine the freshness of an object.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Freshness Metrics}
\label{sec:freshne_metrics}

As discussed in \ref{r:express_freshness}, freshness can be considered with several indications and nuances.
There is no unified definition of data freshness or common freshness metrics.
These rather depend on specific use cases and system requirements
While freshness extractions on value-based divergence are only really suitable for numerical values,
to measure the deviation from a base item, time-based freshness on the other hand can be applied to arbitrary data types and can therefore be used 
in a more general notion.\\
Because the perception of freshness is rather subjective and depends on the use case, the time-based constraints are often still not sufficient for very frequently
updated replicas.
The accuracy would differ greatly when one node has received an update within the last minute and might be considered fresh, but this one particular 
update might have changed the entire table. 
This would then rather reduce the freshness to a simple version-comparison and allow questions such as, "if it exists, how did the data item look like roughly one minute ago".
Admitting that this might be desirable for some use cases we want to extend this notion by considering deviations from the primary copy as well.\\

Hence, we propose that users can specify their tolerated level of freshness in a variety of ways. 
All proposed metrics will apply filters based on the abstract equation in \ref{eq:abstract_freshness}. 
Which essentially validates per replica whether it can be used for the tolerated freshness-level $\delta$ in regard to a given data object $d$.
Where $\delta$ can be associated with any metric described in the following definitions.

\begin{equation} \label{eq:abstract_freshness}
    F(d, \delta)=
        \begin{cases}
            \text{true } & \text{if } f(d) \geq \delta\\
            \text{false } & \text{otherwise }
        \end{cases}
\end{equation}



Given that the freshness filter function $F(d, \delta)$ is valid for all described metrics, the concrete freshness determination individually defined as $f(d)$, 
varies among the use cases.
In general this function is defined to return a specific level of freshness for a particular data object $d$. 
Where the object $d$ is available on all replicas and can vary in terms of freshness, due to different update times.
While the individual freshness function returns a calculated freshness the filter function will remove any replica that does not meet the designated freshness-level $\delta$. 
We only require that $\delta$ and the return value of $f(d)$ are of comparable types.


\begin{description} \label{desc}
    \item [Absolute Timestamp]  A timestamp can be directly specified as a lower bound threshold, during replica comparison. Identifying all replicas that have been updated 
    more recently than the specified timestamp, to be considered during the selection process.\\
    The greater a timestamp the younger it is, respectively also the higher a timestamp is the fresher it is.
    With this function the freshness of a data item $d$ is directly returned as the commit timestamp when this object has been written.
    As mentioned in \ref{sec:data_replicas}, the commit timestamp $t(d)$ can be referred to as the current state of this replica and is associated with the commit time 
    of the transaction within this operation has been applied to the corresponding primary copy.

    \begin{equation} \label{eq:timestamp_freshness}
        f(d) = t(d)
    \end{equation}

    In this case $\delta$ is a user specified timestamp $t_{timestamp}$ and consequently compared against $t(d)$ to verify the if the freshness-constraints are met.


    \item [Absolute Time Delay] Any delay can be useful to intuitively specify the accepted level of freshness without explicitly specifying a timestamp as a lower bound.
    This function rather allows to specify a time delay based on the current time $t_{now}$. 
    This metric therefore allows specifying freshness in respect to the current time. Resulting in recently updated replicas considered to be fresher than others. 
    Although not directly specified as a timestamp, an absolute time delay will still generate a timestamp for comparison to be used in $\delta$.
    The delay is simply subtracted from $t_{now}$ to again generate $\delta$ as a lower bound timestamp used for comparison.
    The freshness evaluation is therefore equal to the equation \ref{eq:timestamp_freshness} and provides a different approach to specify the tolerated freshness.
    Both construct a timestamp that enacts as a lower bound of acceptable placements.
    This can be used as a filter to check for each candidate placement that it is fresher or respectively has received a state where its commit time is newer 
    than the lower bound. If not, the placement is removed from the list of possible candidates. 



    \item [Relative Time Delay] 
    Although, an absolute time delay is useful in some cases by defining its freshness based on the current point in time, 
    it might lose some detail and could filter some replicas that in some scenarios are actually useful.
    If e.g. an object has not been modified for a few hours and although the replicas might already be up-to-date, they will not be considered when specifying 
    an absolute time freshness that accepts the last hour.
    This is also true if the secondary replica is not up-to-date yet and is still in the process of convergence.
    Disregarding its state it will be avoided since its current commit timestamp is out of bound of the specified time delay.
    Although intuitively these replicas are considered rather fresh in respect to its primary copy.\\
    Therefore, if we merely want to observe how much a secondary might deviate from the primary in terms of the update timestamp we need new metric.
    We therefore also need to specify the accepted level of freshness on the basis of divergence from its eagerly replicated counterpart and therefore provide a relative time delay
    used during comparison.\\
    With this metric the specified relative time delay can directly be used as $\delta$. The freshness function $f(d)$ described in \ref{eq:relative_freshness}
    will essentially compare the current commit timestamp $t(d)$ against its primary replica $t(d_{primary})$.
        \begin{equation} \label{eq:relative_freshness}
            f(d) = t(d_{primary}) - t(d)
        \end{equation}

    Again if the calculated deviation from the up-to-date node is within bound of the specified delay in $delta$, this replica is accepted.

    \item [Replica Deviation] Although the first three metrics already provide some granularity to consider different nuances of freshness, we 
        do not yet involve the number pf pending updates to any replica or can differ between the number of modifications each replica has received yet.
        For objects with a comparably high update frequency the notion of timeliness can hardly be utilized to make an assumption on the freshness.
        Therefore, we again want to provide another possibility to allow the specification, based on the divergence between primary and secondary.

        This freshness can be specified by a freshness index as proposed by \cite{rohm:2002}.
        This ratio can be evaluated and consequently generated based on the number of modifications the primary and 
        secondary copy deviate from one another. Where $m(d)$ is defined by the number of modifications a data object $d$ has ultimately received on a given replica
        and $m(d_{primary})$ as the corresponding up-to-date copy to compare against. 

        Although a freshness index does not intuitively provide an observable threshold at first glance, it indicates how accurate a 
        given replica $d$ is in respect to the number of modifications of an up-to-date version $d_{primary}$.

        This \textbf{Modification Deviation} is defined in \ref{eq:modification-deviation}.
        \begin{equation} \label{eq:modification-deviation}
            f(d) = \frac{m(d)}{m(d_{primary})},  with f(d) \in [0,1]
        \end{equation}
        
        Generally this describes how far behind an outdated replica is compared to the primary version.
        It can also be used during when multiple tables are \emph{JOIN}ed.
        We can use the joint number of modifications and compare it against the current number of update transactions to give a joint accumulation.

        Since it also might be desirable to specify a freshness index but to consider a time deviation as described in ~\cite{voicu:2010,hennemann_sw_2021}
        the freshness function can be adjusted as needed to also compare the effective commit timestamps as a \textbf{Commit Time Deviation}.
        Since we ensure that the commit timestamp of a primary replica will never be greater than its eager counterpart. We can define the time deviations as an index that is 
        generated with $t(d_{primary})$ being the commit timestamp of the up-to-date replica and $t(d)$ the timestamp of the possibly outdated replica.


        \begin{equation}
            f(d) = \frac{t(d)}{t(d_{primary})},  with f(d) \in [0,1]
        \end{equation}

\end{description}

All the mentioned freshness metrics can be used to compare different replicas that contain a data object $d$ and filtering them based on the provided function $F(d, \delta)$.
This allows to specify the tolerated level of freshness $\delta$ from a type $\tau \in \{TIMESTAMP, TIME-DELAY, INDEX\}$ to be used within the query specification.

Although as mentioned in section \ref{sec:data_replicas} some engines within a polystore might be more suitable for up-to-date replicas than others,
we don't limit the possibility of different freshness metrics to a subset of stores. Hence, every store is able to uniformly work with all levels of freshness.
The Data Freshness shall always be evaluated within the polystore layer and is then used to compare the tolerated value against possible candidates that might fulfil such a request 
(see \ref{r:read}).






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Update Propagation}
\label{sec:propagation}

To generally allow a system to handle transactional and analytical workload in parallel, we need to reduce occurring locking situations,
such that write and read operations do not drastically interfere with each other.
Since a polystore system acts as an abstraction layer on top of the encompassed stores, we can leverage it to act as a coordination service 
allowing us to restrict the eager replication and locking mechanisms to the primary nodes alone.
Given the multiple versions described in section \ref{sec:data_replicas} we could consequently decouple primary transactions from 
secondary transactions. In that sense any modification to an object will now only target and lock its primary copies which are labeled as \emph{up-to-date}.
The secondary nodes could then be read without any further locking by a user transaction. 
Nonetheless, we need an approach how these outdated stores can converge their state towards the state of their primary copy.
Otherwise, we would have entirely outdated stores that would remain in their current state and users using these store for querying will always read stale data. 


\subsection{Refresh Strategies}
\label{sec:refresh_strategies}

Since we have no longer access to an eager replication we need the possibility to apply the changes lazily to any outdated replica.
Depending on the use case there might be different approaches needed to fulfil certain requirements.

We therefore propose the following strategies to apply pending updates to the outdated nodes.

\begin{description}
    \item[Immediate Execution] This approach mainly pursues the decoupling of one single eagerly applied transaction, to two subsequent transactions.
    While the eagerly applied modification is executed synchronously it is bound by the slowest performing node in a setup.
    That is why with a lazy update propagation we only have to wait for the up-to-date replicas to finish the transaction.
    Since these can be strategically placed on stores that are suitable for transactional workload, they are assumed to apply the operation faster.
    After this primary transaction has then committed and the locks are released, an asynchronously executed secondary update transaction can be executed, 
    applying the updates to the outdated nodes.\\
    Since these secondary transactions are merely executed with a small processing delay and assuming that the update queue might not be very large and 
    updates can be applied right away, the outdated nodes intuitively will not deviate much from its primary partner.

    \item[On-demand Refresh] Since a regular queue will not consider priorities, and we still have to obey the execution constraints of the primary transaction (see \ref{sec:consistency_concept})
    we always need to preserve the execution order of the primary transaction. Depending on the size of the update propagation-queue some replicas might stay outdated longer than others.
    That is why an on-demand approach is necessary to refresh outdated nodes at once and bring them up-to-date.
    Applying such an approach, the queue has to remove all pending updates for that specific replica, to avoid that updates are not applied twice. 
    
    \item[Load-aware] Although an automatically scheduled execution and a manual execution might serve most use cases, it might not be desirable to do so.
    Neither an immediate execution after a primary transaction nor an on-demand triggered refresh, take the system load into account.
    Since polystores consist of several potentially heterogeneous stores they might also differ in terms of their computing resources.
    So while most of the underlying stores could apply the pending changes immediately, some might currently not be capable on handling the additional load and have to 
    be deferred yet again. Despite that this approach might slow down the convergence speed of the updated nodes, 
    it will observe underlying stores and artificially limit the load on the system introduced through the propagation of updates hence keeping the system stable and available.

    \item[Update on Read] So far, all proposed solutions suffer either from additional evaluation overhead or from manual interference. This will limit the overall 
    performance of the system. We could therefore again introduce a decoupled operation that is automatically triggered as soon as an outdated replica has been part of any query. 
    During the freshness-related retrieval of any data object, we obviously identify that this is indeed an outdated node and can directly schedule an update propagation for it. This propagation is then
    executed asynchronously after the initial read-operation has finished. This would reduce the additional caching, and avoid storing 
    information as which updates need to be applied on which node.\\
    However, the downside of this approach is, in highly transactional environments with heavy write- and read-operations, that the outdated node would always be marked as 
    needing an update. This would schedule an update, although it might have already been scheduled by another strategy. 
    To mitigate this, the strategy could be enhanced even further by allowing a centrally defined configuration threshold, that validates how much an outdated replica deviates from its priamry. 
    This assumes that if an outdated replica has been read and is above the centrally configured threshold, that no update propagation will be automatically scheduled.
    This will avoid permanent scheduling of an update for every freshness-related read access.

\end{description}





\subsection{Refresh Operations}

The Update Propagation generally refers to the refresh operation that transforms possibly outdated objects towards an up-to-date state.
Disregarding the described Refresh Strategies from section \ref{sec:refresh_strategies} we need to converge the outdated replicas towards their 
primary copies.
There are several possibilities to achieve this and a system can choose to implement each of these cases in various ways. However, each implementation comes with its
own trade-offs.
We have several possibilities how to handle and propagate the updates.
Since we assume that every write-operation needs to go through the polystore-layer, we can easily keep track which operations have been applied to the primary node.
To ensure the overall consistency we require that the operations are executed in correct execution order and therefore need to apply all pending changes as they have been applied at the priamry site.
This imposes a natural execution order of any item in the queue to be delivered to the secondaries.

We need to define how executed operations are tracked and how they then apply those operation to the replicas. 
Therefore, we propose the following approaches: \\

\begin{description}
    \item [Change Data Capture]
     As the name suggests the \emph{Change Data Capture} (CDC) approach aims to preserve every modification that has been applied to the primary node, cache it and ultimately
    apply it to all relevant outdated nodes. 
    The idea of this approach is that all changes including the data could be temporarily stored either in-memory or persisted onto a disk.
    The choice where to store it depends on the individual consistency-availability requirements and will not be part of this discussion.\\
    For the CDC-Algorithm we consider that during an active transaction all changes will be tracked and written into a Firt-In-First-Out (FIFO) queue.
    Since this is only a preliminary step we will conveniently call this capture-collection: \emph{capture-queue}.
    As operations are being executed, each change along with its data and the corresponding parent transaction is stored within this capture-queue.
    Although we could simply capture the executed statement in a Write-Ahead-Log (WAL) and re-execute it on the underlying stores, we now benefit from the polystore layer.
    Since possibly existing functions or constraints might have already been evaluated at runtime. Thus, we can save further computations and store the end result that is pushed-down directly
    to the designated stores. As soon as this transaction has been successfully committed, all entries in this capture-queue are further enriched with the respective commit timestamp of their parent transaction.
    Afterwards the corresponding entries in the capture-queue are added to an actual central replication queue containing the pending updates.
    For each designated replica that should receive this update an individual entry is created inside this queue. 
    Each entry is accompanied by its parent transactions ID, the commit timestamp as well as the data to be replicated.
    Since we do not need to store the data $n$-times for $n$ replicas determined to receive the data, we can simply link each replication item in the queue to its 
    correspodning replication data, which is stored separately. 
    Since all entries in this final replication-queue are ordered in respect to the execution order of the original transaction,
    we have ensured that the operations are executed in order to conerge to the same state as its primary copy.\\ 
    Finally, if the transaction aborts, all active entries in the initial capture-queue can be removed due to their association with the parent transaction.\\
    
    \todoMissing{In Implementation, state that e.g. for update operations this is broken down into operations applied to each store. }




    \item [Primary Data Snapshot]
    Although CDC will correctly recreate any secondary replica, it will lose its efficiency when there are more modifications to apply to secondaries that there 
    have been totally applied at the priamry site. Although it would still produce the correct result it could be further optimized without replicating operation by operation 
    until the replica has converged.\\
    Therefore another propositions could be the usage of a primary-copy approach. 
    Intuitively this would allow to simply snapshot the entire state of a matching primary node to be copied onto the traget replica.
    During this copy we only need the current commit timestamp of the primary and snapshot the current state of the respective data object. 
    This could be done simply by executing a read-only transaction to retireve the current state of the primary replica.
    Since the snapshot itself will have no real impact on the primary node, we can continue to use it for all operations.
    Because the secondary replica will be recreated from scratch, querying it will result in an incorrect state. 
    Therefore it cannot by activiely used by any freshness-related queries.
    Hence, we have to refrain from providing this replica as a possible candidate in the retireval process, and lock it entirely 
    until everything has been processed and the replica is equal to the snapshot. After it has been applied we can now update the commit timestamp of the replica
    with the timestamp retrieved alongside the snapshot, to mark this refresh as successfull.\\
    Despite that this snapshot-copy will again result in a correctly updated secondary node, it is not suitable for very large data sets to copy.  
    For one, depending on the refresh strategiy proposed in section \ref{sec:refresh_strategies}, it could be triggered too frequently and would constantly lock the 
    secondaries. Additioanlly, a complete copy of a data set, takes time, which removes the replica from the potenial candidate replicas to be used within retrieval.\\
    To avoid these locking situations, we could further adapt this algorithm to create a temporary shadow replica while the copy process is in place.
    With this we could recreate an entirely new replica on the basis of the snapshot, which would still allow acessing the old outdated node.
    Allthough possibily more outdated data is now retrieved, and the data footprint is temporarily increased, this replica can still continue to serves freshness-queries 
    since it will not need to be locked.
    Finally, when the process has finished we only need to apply a lock during takeover time to ensure the consistency. During this short timeframe, the old replica is dropped
    and the newly created hidden shadow replica is now activated, making it an official replica to be used.



    \item [View Materialization]
    Along the idea of the \emph{Primary Data Snapshot}, the materialization of views could also help to reduce the number of statements necessary to create new levels of freshness.
    Since materialized views are by nature considered to be precomputed-snapshots of data objects, we can simply leverage these semantics to create different versions of data, 
    represented by indvidual views.
    Because views are common in most databases there are an easy to use access without implementing an entire refresh algorithm.\\
    In contrast to the benefits described in section \ref{sec:data_replicas} we now indeed need to artificially create new versions.
    However, instead of replicating the data operation-wise to another store, we can simply omit to create replicas that become outdated and create materialized views on this 
    stores instead. Hence, we are left with at least one true up-to-date replica and several outdated replicas, which are repserented by views created on the underlying stores.
    Due to their flexibility we can decide per use case which degree of freshness a view supports. 
    Analogously to the aformentioned approach, anytime a propagation or refresh operation is being executed a new materialized view is generated on basis of the up-to-date replica.
    This also ommits replicating single operations entirely, hence no bookkeeping of the queued updates is necessary. 
    The only needed reference would again be the commit timestamp of the primary node. 

    %As with the handling of internal partitions this can also be done entirely in the background. This limits the locking time to a minimum and is therefore only really 
    %necessary when switching the outdated with a refreshed view.
    %Although, one of the biggest downsides of materialization is the large additional resource consumption on storage, it can be neglected for polystore systems, since they are
    %inherently distributed and would otherwise store the entire content of the table. Therefore, it would even slightly reduce the data footprint on outdated placements.

\end{description}


While all of these approaches can be used to replicate and refresh the data on outdated nodes,
they all come with their own set of trade-offs and might be used in different scenarios. 
%So given that the change-data-set is larger than the total set of operations that have been applied to the priamry-replica it looses efficiency. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Transactional guarantees - Locking - Isolation Level}
\label{consistency}

To support the overall consistency of the system we have to ensure that all updates are correctly serialized and applied to the system.

Polypheny for one ensures global atomicity by using the two-phase commit (2PC) protocol and for correct isolation treatment
strong strict to phase locking (SS2PL). Currently, all placements are eagerly updated and locked whenever a write-operation is executed.
We need to loosen that constraints to increase update times and still ensure all transactional guarantees. The locking mechanism should therefore 
now only consider primary nodes. In that way we still can utilize secondaries with slightly outdated data for reads.\\

To ensure that the overall correctness of the system is maintained we will restrict the solution to allow freshness related parameters solely
for read-only transactions. For every update-transaction which considers freshness of any kind, consistency cannot be guaranteed since we might read outdated data
and use this data to write into another table. Therefore, we propose to enrich  the transactional context of Polypheny with a flag that states if any read-operation 
considers freshness. If this is the case, the update-transactions needs to be aborted.\\


Another important point to note is the referential integrity of outdated tables. We already ensured that outdated data cannot be used within update-transactions to 
ensure consistency and therefore guarantee correctness. However, for read-only transactions it should still be possible to join any tables while considering 
a desired level of freshness. 
Since it is rather complex and would result in refactoring the core of Polypheny-DBs, we currently refrain from supporting MVCC.
Hence, it might occur that cumulative reads on multiple objects might return incomplete results, since a specified freshness
level can be entirely different on two tables which makes it hard to even compare freshness levels among different objects. However since a user is willing to access 
stale data anyway this is a known risk and thus can be accepted.\\

Finally, as introduced in \ref{replication} the \emph{Refresh-Lock} is a newly introduced lock which holds the same characteristics as a regular lock on an object.
However, it shall only be applied whenever a refresh operation is currently in place. This way the routing mechanism can avoid sending any queries to that placement
for reads or a new refresh operation, which might have been triggered manually by an user.

To ensure the correct execution of refresh operations every placement that is outdated will receive an independent propagation- or refresh-transaction.
The serialization order of the updates to be executed is the commit order of the initial write transaction.
These can also be refreshed and updated independently which not only again reduces the total update time but also eases rollback scenarios.
Otherwise, we might need to define complex countermeasures to undo certain refreshes if one store was already refreshed but another has failed.

\todoMissing{List what building blocks are necessary to compose a freshness concept. Check if needed as separate section}

\todo{Resort}
The locking is done logically within the polystore layer and locks the entire table.
Since we want to establish the freshness comparison ob objects based on the Data Placement respectively each individual partition placement, the locking mechanism
needs to be adapted to allow locking not only on table-level but rather on a partition level. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Freshness-aware Read Access}
\todoMissing{with the Read semantics -  we are now able to}

\todoMissing{However, we want to include primary nodes for reads as well }

With the description of \ref{express} how to explicitly express freshness, users have the ability to hint or even guide the routing process to select placements 
based on their desired freshness.\\
Users can choose to specify any tolerated level of freshness. The routing process analyzes this specification and gathers all placements which fulfil the necessary 
requirements by preferring secondary nodes labeled as outdated.
In case no secondary node can fulfil the request a primary placement is used to serve the query since they are not exclusive to receive updates such in other systems,
consequently utilizing all source available to the system.
Since only few systems presented in \ref{r:read} provided this functionality the routing process can be extended to support load balancing. 
With the proposed adapter background analysis the router can observe if any selected placement might be overloaded and therefore chooses to route 
queries to a different available location. \\



For the overall freshness guidance an extension of PolySQL is necessary.
Along the description in \ref{express} users can choose to select any of the specifications to guide the system.
\begin{verbatim}
SELECT * FROM dummy 
[ WITH FRESHNESS [ <TIMESTAMP> | <DELAY> | <PERCENTAGE> ] ];
\end{verbatim}
As an extension we also propose to omit the specifications entirely and only state to the system that any freshness level is acceptable or the system central
configured default, to further speed up queries.



As mentioned in \ref{consistency} since update transactions don't allow any usage of freshness metrics to ensure consistency, solely read-only transactions are allowed.
Hence, for read-operations on outdated nodes and data, locks can be omitted entirely since you will read stale data anyway.
We therefore only have to validate that no refresh-lock is currently in place.




\todoMissing{Describe Process, Replicate Data to receive several version, reduce locking and decouple primary transactions, during query processing extract the required level of freshness
and filter on all nodes that can fulfil this freshness. Then pick a suitable one to provide the query.}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Consistency}
\label{sec:consistency_concept}
Move from Strong consistency to eventual consistency 
have a loose form of consistency constraints that we need to comply with, in order to still provide response times while sacrificing consistency.

In general two requirements can be defined that have to be fulfilled to
guarantee correctness in respect of freshness. For one, transactions that refresh stores need to be executed on all read-only nodes in the same serialization order as the 
original update transaction. Secondly all queries of a read transaction must access data with the same freshness. \todo{Cite}

\todoMissing{Several possibilities to lock, only on master, on frshenss no lock at  all but all dirty reads lock primaries that they do not get refreshed but therefore stay consistent at their freshness}

\todoMissing{Introduce constraints on freshness and DML operations within the same transaction}

\todoMissing{When a refresh transaction  is executed no more locks on secondary nodes can be applied, if there is still a shared lock on that outdated node
they will first be commited before the refresh takes place  (avoid deadlock) if we do not hinder the system to build more shared locks refresh operations might starve}

\todoMissing{Add this to requirements when talking about what is needed to provide freshness-aware data management}
This is certainly we still have to comply with the acid properties. 


As suggested in \ref{r:replication} there exist several techniques how outdated nodes can be updated lazyily in the context of freshness.
Most of these presented distributed architectures follow a primary-copy approach for master-driven replication to all their secondary replicas.
This eases the control and flow of data. Although in the proposed works some systems allowed to access read-only copies directly
with Polypheny-DB we always have a single point of entry which can vaguely be compared as the poly-layer acting as a master node
when distributing or even routing queries. Hence, Polypheny acts as the master anyway. However since any requests have to pass through the poly-layer
we have full control how and where queries need to be routed to allowing us to selectively route read-operations to outdated and up-to-date stores alike.
This enables Polypheny-DB to take full control how different levels of data freshness are being accessed.

With this abstraction layer on top of the stores we can leverage the DBMS to act as a coordination service allowing us to loosen the eager replications and 
locking mechanisms to be restricted to the primary nodes only. Currently, one has to wait until an update is persisted everywhere and are therefore
dependened on slowest performing store.
To mitigate this behaviour we want to decouple the commonly used update transactions and logically divide it into a total of three separate transactions types
to allow deferred refreshments of objects.
\begin{itemize}
    \item \textbf{Update transaction}: Consequently are write operations that are targeted on primary placements only.
    \item \textbf{Propagation transaction}: Belong to a refresh operation updating the data freshness of an outdated secondary node which is 
    executed automatically by the system.
    \item \textbf{Refresh transaction}: A propagation transaction which is manually triggered by a user to refresh the data on an arbitrary outdated node.
\end{itemize}

Although, logically being used differently they are technically executed with the same capabilities and only really separate in terms of locking.
They are rather used as a clear indication on which part of the process is referred.

These changes will also have an impact on the partition distribution constraints. For vertically and horizontally partitioned 
entities this means that all constraints on the number of available column-placements can only be considered for primary update stores. Otherwise, this could harm
the consistency of the system. For nodes labeled as outdated any variation is possible.


\todoMissing{In our proposal primary-copies can still be used during proposal }

\section{Placement constraints}
\todoMissing{What happens for data DDL operations, to they neglect the freshness}