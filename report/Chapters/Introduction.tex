% !TEX root = ../Thesis.tex
\chapter{Introduction}
\label{c:intro}

Over the last decade, cloud computing has become a crucial and central part in the industry \cite{claremont:2005}.
This technological advancement in infrastructure layouts allows companies to rent software as well as hardware resources to
build and compose their entire data center virtually in the cloud,
without having to invest in expensive hardware and location to maintain their compute resources
solely on premise. These providers usually maintain different distributed data centers 
across the globe and provide private or shared access on their available resources according
to their Service Level Agreement (SLA). Such guarantees in quality-of-service (QoS) usually 
include elastic up- and down-scaling of rented resources and a high degree of availability 
which is achieved by replicating data across different regions \cite{terry:2013}. 

Ever since the rise of the \emph{Big Data} era, these providers are faced with continuous
and rapidly growing heterogeneous datasets. These are accompanied by widely varying requirements 
and characteristics for data driven applications. This increased the need to leverage custom-tailored 
database systems to gain meaningful insights on their data as quickly as possible.

To efficiently process and extract relevant information out of these data silos new systems
have emerged, which exactly aim to provide a solution for these new 
demands on varying workloads and ever-growing data sources.

Such novel Polystore systems aim to combine several physical data engines to enable various
new possibilities, to provide the best response times for each use case by exploiting
the key benefits of each engine \cite{stonebraker:2005}. Although these data management systems are inherently
built to process heterogeneous data with high throughput, the amount of produced data
still continues to grow.
Therefore, the importance of accessing the right data efficiently is crucial
for organizations to stay economical and competitive. Cloud providers which offer such systems consequently 
need reliable functionality to manage these large volumes of data to not waste useful computations and time when users
try to retrieve relevant data \cite{levandowski2013}.
To comply with their QoS requirements and sufficiently provide acceptable access times for their services, 
they have come up with data freshness strategies as an important part of data management in a
distributed setup. 
The freshness in such a case reflects how current and up-to-date a data item is.
Since not all applications pursue the same goals, they often depend on different levels of freshness which can be used to identify a well-suited location
to retrieve the data. This ultimately mitigates the need to always update every existing data replica to the most recent state and defer changes until
it is processable by the underlying system.
Such delayed updates allow for much higher throughput and increased performance, in use cases were also slightly outdated data 
is acceptable.

\section{Goal?}

\section{Contribution}
This report contributes as a preparation for a masters thesis. To identify necessary requirements to implemented and design possible 
freshness variations in a polystore system. Several aspects of previous published research work is compared and considered when adapting it to polystore systems.

\section{Outline}
This report is structured as follows:
In chapter \ref{c:related} the foundations and concepts of data freshness characteristics are presented.
We give an overview over the current state of research in the field of data freshness.
Chapter \ref{c:propositions} describes the necessary requirements to introduce the notion of freshness inside a polystore system. Additionally, it proposes and discusses
possible approaches how to implement them in Polypheny-DB.
While chapter \ref{c:evaluation} focuses on possibilities how to ensure correctness and measure the performance of an implementation, including all necessary prerequisites.
Finally, chapter \ref{c:plan} concludes the report by defining individual workpackages according to the proposed
implementations and gives a coarse roadmap how the project will be structured and carried out.


